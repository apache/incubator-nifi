<!DOCTYPE html>
<html lang="en">
    <!--
      Licensed to the Apache Software Foundation (ASF) under one or more
      contributor license agreements.  See the NOTICE file distributed with
      this work for additional information regarding copyright ownership.
      The ASF licenses this file to You under the Apache License, Version 2.0
      (the "License"); you may not use this file except in compliance with
      the License.  You may obtain a copy of the License at
          http://www.apache.org/licenses/LICENSE-2.0
      Unless required by applicable law or agreed to in writing, software
      distributed under the License is distributed on an "AS IS" BASIS,
      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
      See the License for the specific language governing permissions and
      limitations under the License.
    -->
    <head>
        <meta charset="utf-8" />
        <title>CreateHadoopSequenceFile</title>

        <link rel="stylesheet" href="../../../../../css/component-usage.css" type="text/css" />
    </head>

    <body>
        <!-- Processor Documentation ================================================== -->
        <h2>Description:</h2>
        <p>This processor is used to create a Hadoop Sequence File, which essentially is a file of key/value pairs. The key 
            will be a file name and the value will be the flow file content. The processor will take either a merged (a.k.a. packaged) flow 
            file or a singular flow file. Historically, this processor handled the merging by type and size or time prior to creating a 
            SequenceFile output; it no longer does this. If creating a SequenceFile that contains multiple files of the same type is desired,
            precede this processor with a <code>RouteOnAttribute</code> processor to segregate files of the same type and follow that with a
            <code>MergeContent</code> processor to bundle up files. If the type of files is not important, just use the 
            <code>MergeContent</code> processor. When using the <code>MergeContent</code> processor, the following Merge Formats are 
            supported by this processor:
        <ul>
            <li>TAR</li>
            <li>ZIP</li>
            <li>FlowFileStream v3</li>
        </ul>
        The created SequenceFile is named the same as the incoming FlowFile with the suffix '.sf'. For incoming FlowFiles that are 
        bundled, the keys in the SequenceFile are the individual file names, the values are the contents of each file.
    </p>
    NOTE: The value portion of a key/value pair is loaded into memory. While there is a max size limit of 2GB, this could cause memory
    issues if there are too many concurrent tasks and the flow file sizes are large.

        <h2>SSL Configuration:</h2>
        <p>
            Hadoop provides the ability to configure keystore and/or truststore properties. If you want to use SSL-secured file system like swebhdfs, you can use the Hadoop configurations instead of using SSL Context Service.
            <ol>
                <li>create 'ssl-client.xml' to configure the truststores.</li>
                <p>ssl-client.xml Properties:</p>
                <table>
                    <tr>
                        <th>Property</th>
                        <th>Default Value</th>
                        <th>Explanation</th>
                    </tr>
                    <tr>
                        <td>ssl.client.truststore.type</td>
                        <td>jks</td>
                        <td>Truststore file type</td>
                    </tr>
                    <tr>
                        <td>ssl.client.truststore.location</td>
                        <td>NONE</td>
                        <td>Truststore file location. The mapred user should own this file and it should have default permissions.</td>
                    </tr>
                    <tr>
                        <td>ssl.client.truststore.password</td>
                        <td>NONE</td>
                        <td>Truststore file password</td>
                    </tr>
                    <tr>
                        <td>ssl.client.truststore.reload.interval</td>
                        <td>10000</td>
                        <td>Truststore reload interval, in milliseconds</td>
                    </tr>
                </table>

        <p>ssl-client.xml Example:</p>
        <pre>
&lt;configuration&gt;
  &lt;property&gt;
    &lt;name&gt;ssl.client.truststore.type&lt;/name&gt;
    &lt;value&gt;jks&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;ssl.client.truststore.location&lt;/name&gt;
    &lt;value&gt;/path/to/truststore.jks&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;ssl.client.truststore.password&lt;/name&gt;
    &lt;value&gt;clientfoo&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;ssl.client.truststore.reload.interval&lt;/name&gt;
    &lt;value&gt;10000&lt;/value&gt;
  &lt;/property&gt;
&lt;/configuration&gt;
                    </pre>

        <li>put 'ssl-client.xml' to the location looked up in the classpath, like under NiFi conriguration directory.</li>

        <li>set the name of 'ssl-client.xml' to <i>hadoop.ssl.client.conf</i> in the 'core-site.xml' which HDFS processors use.</li>
        <pre>
&lt;configuration&gt;
    &lt;property&gt;
      &lt;name&gt;fs.defaultFS&lt;/name&gt;
      &lt;value&gt;swebhdfs://{namenode.hostname:port}&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
      &lt;name&gt;hadoop.ssl.client.conf&lt;/name&gt;
      &lt;value&gt;ssl-client.xml&lt;/value&gt;
    &lt;/property&gt;
&lt;configuration&gt;
                  </pre>
        </ol>
        </p>
</body>
</html>

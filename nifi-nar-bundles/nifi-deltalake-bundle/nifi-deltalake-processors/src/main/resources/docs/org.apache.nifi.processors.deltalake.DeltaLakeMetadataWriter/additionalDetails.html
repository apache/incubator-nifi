<!DOCTYPE html>
<html lang="en">
<!--
  Licensed to the Apache Software Foundation (ASF) under one or more
  contributor license agreements.  See the NOTICE file distributed with
  this work for additional information regarding copyright ownership.
  The ASF licenses this file to You under the Apache License, Version 2.0
  (the "License"); you may not use this file except in compliance with
  the License.  You may obtain a copy of the License at
      http://www.apache.org/licenses/LICENSE-2.0
  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
-->
<head>
    <meta charset="utf-8" />
    <title>UpdateDeltaLakeTable</title>

    <link rel="stylesheet" href="../../../../../css/component-usage.css" type="text/css" />
</head>
<body>
	<h2>High level overview</h2>
    <p>This processors main task to create or keep up to date a Delta table within a directory full of parquet files.
        This directory (with full of parquet files) can be on the local filesystem or in an Amazon S3, Azure or GCP cloud storage.
        Under the hood this processor uses the <a href="https://docs.delta.io/latest/delta-standalone.html">
        Delta Standalone</a> library to generate the delta table. This processor also capable to process other processors output file
        and upload it to the data store.</p>
	<h2>Concept</h2>
    <p>First the processor checks for input file. If there is an input file, the processor copy that file into the data directory.
        After that the processor reads up the parquet file names from the data directory and the already added file names from the delta log.
        It compares them and creates a list of files which needs to be added or removed from the Delta log.
        At the end it commits the update in a single transaction. By this the Delta table will always be up-to-date.
        The Delta table is located in a separate _delta_log directory within the directory containing te parquet files.</p>
	<h2>Configuration</h2>
    <h3>Partition Values</h3>
    <p>This processor supports partition values. You can partition a Delta table by columns.
        Partition columns can be given in the <code>Parquet partition columns</code> property. Each partition should be separated by comma.
        It's the users' responsibility to put the parquet files into partitioned directories. <br>
        Example value: <code>year,customer</code> </p>
    <h3>Input file</h3>
    <p>This processor can process other processors output file. For example, it can process the PutParquet processors output file.
        The processor processes one file at a time. To achieve that just connect a processors output to this processor. If the input file is partitioned,
        the <code>Parquet partition columns</code> and the <code>Partition Values Of The Input File</code> has to be filled.</p>
    <h4>Example</h4>
    <table>
        <tr>
            <th>UpdateDeltaLakeTable input property</th>
            <th>PutParquet output attribute name</th>
        </tr>
        <tr>
            <td>Parquet Partition Columns</td>
            <td><code>year,customer</code></td>
        </tr>
        <tr>
            <td>Partition Values Of The Input File</td>
            <td><code>2020,555</code></td>
        </tr>
    </table>
    <h3>Parquet structure</h3>
    <p>The structure of the parquet files needs to be configured in a JSON format. It can be declared as text or by giving a path to a JSON file in the local filesystem.</p>
    <p>Example of a valid JSON structure:</p>
    <pre>
{
    "type": "struct",
    "fields": [
        {
            "name": "year",
            "type": "integer",
            "nullable": true,
            "metadata": {}
        },
        {
            "name": "month",
            "type": "integer",
            "nullable": true,
            "metadata": {}
        },
        {
            "name": "day",
            "type": "integer",
            "nullable": true,
            "metadata": {}
        },
        {
            "name": "sale_id",
            "type": "string",
            "nullable": true,
            "metadata": {}
        },
        {
            "name": "customer",
            "type": "string",
            "nullable": true,
            "metadata": {}
        },
        {
            "name": "total_cost",
            "type": "float",
            "nullable": true,
            "metadata": {}
        }
    ]
}
    </pre>
    <h3>Hadoop Configuration</h3>
    <p>Hadoop Configuration can also be used to configure the cloud storages. First a Configuration will be created based on the entered configuration file.
        After that, if any cloud configuration is presented within the processors attrinutes, that will upsert that attribute.
    </p>
    <p>Example of an S3 Configuration:</p>
    <xmp>
        <configuration>
            <property>
                <name>fs.s3a.access.key</name>
                <value>ACCESS_KEX</value>
            </property>
            <property>
                <name>fs.s3a.secret.key</name>
                <value>SECRET_KEY</value>
            </property>
        </configuration>

        </xmp>
    <h2>Attributes</h2>
    <table>
        <tr>
            <th>Name</th>
            <th>Default Values</th>
            <th>Allowable Values</th>
            <th>Description</th>
        </tr>
        <tr>
            <td>Parquet storage location</td>
            <td>Local Filesystem</td>
            <td>Local Filesystem<br>
                Amazon S3<br>
                Microsoft Azure<br>
                GCP</td>
            <td>Select where the data is located, on local filesystem or in the cloud</td>
        </tr>
        <tr>
            <td>Hadoop Configuration Resources</td>
            <td></td>
            <td>List of filenames, separated by comma<br>
            <td>If presented, the processor will try to create a Hadoop Configuration from it</td>
        </tr>
        <tr>
            <td>Data path on local filesystem</td>
            <td></td>
            <td></td>
            <td>The directory containing the parquet files. Can be absolute or relative</td>
        </tr>
        <tr>
            <td>Parquet schema location</td>
            <td>Text Input</td>
            <td>Text Input<br>
                Local file path</td>
            <td>Choose which format you want to define the parquet files structure</td>
        </tr>
        <tr>
            <td>Parquet schema in JSON</td>
            <td></td>
            <td></td>
            <td>Enter the parquet files structure as text</td>
        </tr>
        <tr>
            <td>Parquet schema file location</td>
            <td></td>
            <td></td>
            <td>Give a relative or absolute path for the parquet files structure</td>
        </tr>
        <tr>
            <td>Parquet partition columns</td>
            <td></td>
            <td></td>
            <td>Partition columns separated by comma</td>
        </tr>
        <tr>
            <td>Partition Values Of The Input File</td>
            <td></td>
            <td></td>
            <td>If the input file is partitioned, this has to be filled with the partition values separated by coma. Example: 2020,555</td>
        </tr>
        <tr>
            <td>S3 Access Key</td>
            <td></td>
            <td></td>
            <td>If you choose Amazon S3 cloud storage you have to provide your S3 access key</td>
        </tr>
        <tr>
            <td>S3 Secret Key</td>
            <td></td>
            <td></td>
            <td>If you choose Amazon S3 cloud storage you can provide your S3 secret key</td>
        </tr>
        <tr>
            <td>S3 bucket URL</td>
            <td></td>
            <td></td>
            <td>Your root buckets URL in Amazon S3</td>
        </tr>
        <tr>
            <td>Data path in S3 bucket</td>
            <td></td>
            <td></td>
            <td>Your data directory path within your Amazon S3 bucket</td>
        </tr>
        <tr>
            <td>Azure Account Key</td>
            <td></td>
            <td></td>
            <td>If you choose Azure cloud storage you can provide your Azure account key</td>
        </tr>
        <tr>
            <td>Azure blob storage name</td>
            <td></td>
            <td></td>
            <td>The blobs name in the Azure storage</td>
        </tr>
        <tr>
            <td>Azure blob storage account</td>
            <td></td>
            <td></td>
            <td>If you choose Azure cloud storage you have to provide your Azure storage account</td>
        </tr>
        <tr>
            <td>Data path in Azure blob</td>
            <td></td>
            <td></td>
            <td>Your data directory path within your Azure blob</td>
        </tr>
            <td>GCP account JSON keyfile path</td>
            <td></td>
            <td></td>
            <td>You can provide your GCP account JSON keyfile path in case of Google Cloud Storage, which has to be on the local filesystem</td>
        </tr>
            <td>GCP bucket URL</td>
            <td></td>
            <td></td>
            <td>Your root bucket URL in Google Cloud Storage</td>
        </tr>
            <td>Data path in GCP bucket</td>
            <td></td>
            <td></td>
            <td>Your data directory path within your Google Cloud Storage</td>
        </tr>
    </table>
    <h2>Output</h2>
    <table>
        <tr>
            <th>Attribute</th>
            <th>Relationship</th>
            <th>Explanation</th>
        </tr>
        <tr>
            <td>number.of.new.files</td>
            <td>success</td>
            <td>Returns how many new files has been successfully added to the Delta table</td>
        </tr>
        <tr>
            <td>number.of.files.removed</td>
            <td>success</td>
            <td>Returns how many files has been successfully removed from the Delta table</td>
        </tr>
        <tr>
            <td>number.of.files.in.the.new.deltatable</td>
            <td>success</td>
            <td>If there were no Delta table, a new table is created and this attribute will contain the number of files the new table has</td>
        </tr>
    </table>
</body>
</html>